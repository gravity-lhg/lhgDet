# Author: Haoguang Liu
# Date: 2022.3.25 08:32 AM
# Email: 1052979481@qq.com
# Github: https://github.com/gravity-lhg

import torch
import torch.nn as nn

class GenTargets(nn.Module):
    '''  '''
    def __init__(self, strides, limit_ranges):
        '''
        args:
            strides: Downsampling ratio for each layer.(a list)
            limit_ranges: The range of the bounding box generated by each layer.(a list)
        '''
        super(GenTargets, self).__init__()
        self.strides = strides
        self.limit_ranges = limit_ranges
        assert len(self.strides) == len(self.limit_ranges), 'dim different between strides and limit_ranges'

    def forward(self, inputs):
        '''
        args:
            inputs: A list, [out, batch_boxes, batch_classes]
                out: A list, [cls_logits, cnt_logits, reg_predict], from ClsCntRegHead
                    cls_logits: A list, len is the num of using layer form fpn, default: 5
                                each element [b, c, h, w]
                    cnt_logits: like cls_logits, each element [b, 1, h, w]
                    reg_predict: like cls_logits, each element [b, 4, h, w]
                batch_boxes: A list, gt_boxes, [b, m, 4], m is num of gt_box in each img
                batch_classes: A list, [b, m]
        return:

        '''
        cls_logits, cnt_logits, reg_predict = inputs[0]
        gt_boxes = inputs[1]
        gt_classes = inputs[2]
        cls_targets_all_level=[]
        cnt_targets_all_level=[]
        reg_targets_all_level=[]
        assert len(self.strides) == len(cls_logits), 'dim different between strides and ClsCntRegHead out'
        for level in range(len(cls_logits)):
            level_out = [cls_logits[level], cnt_logits[level], reg_predict[level]]
            level_target = self._gen_level_target(level_out, gt_boxes, gt_classes, self.strides[level], self.limit_ranges[level])
            cls_targets_all_level.append(level_target[0])
            cnt_targets_all_level.append(level_target[1])
            reg_targets_all_level.append(level_target[2])
        return torch.cat(cls_targets_all_level, dim=1), torch.cat(cnt_targets_all_level, dim=1), torch.cat(reg_targets_all_level, dim=1)
        
    def _gen_level_target(self, level_out, gt_boxes, gt_classes, stride, limit_range, sample_radiu_ratio=1.5):
        '''
        args:
            level_out: [[b, c, h, w], [b, 1, h, w], [b, 4, h, w]]
            gt_boxes: [b, m, 4]
            classes: [b, m]
            stride: int
            limit_range: [min, max]
        '''
        cls_logit, cnt_logit, reg_pred = level_out
        batch_size = cls_logit.shape[0]
        class_num = cls_logit.shape[1]  # c in cls_logit is class_num

        coords = self.fmap2orig_coords(cls_logit, stride).to(device=gt_boxes.device)  # [h * w, 2]

        cls_logit = cls_logit.permute(0, 2, 3, 1).reshape((batch_size, -1, class_num))  # [b, h, w, c] ==> [b, h*w, c]
        cnt_logit = cnt_logit.permute(0, 2, 3, 1).reshape((batch_size, -1, 1))  # [b, h, w, 1] ==> [b, h*w, 1]
        reg_pred = reg_pred.permute(0, 2, 3, 1).reshape((batch_size, -1, 4))   # [b, h, w, 4] ==> [b, h*w, 4]

        h_mul_w = cls_logit.shape[1]

        x_list = coords[:,0]
        y_list = coords[:,1]
        # create left, top, right, bottom offset
        left_off = x_list[None,:,None] - gt_boxes[...,0][:,None,:]  # [1, h*w, 1] - [b, 1, m] ==> [b, h*w, m]
        top_off = y_list[None,:,None] - gt_boxes[...,1][:,None,:]
        right_off = gt_boxes[...,2][:,None,:] - x_list[None,:,None]
        bottom_off = gt_boxes[...,3][:,None,:] - y_list[None,:,None]
        ltrb_off = torch.stack([left_off, top_off, right_off, bottom_off], dim=-1)  # [b, h*w, m, 4]

        areas = (ltrb_off[...,0] + ltrb_off[...,2]) * (ltrb_off[...,1] + ltrb_off[...,3])   #[b, h*w, m]

        off_min = torch.min(ltrb_off, dim=-1)[0]    # [b, h*w, m]
        off_max = torch.max(ltrb_off, dim=-1)[0]    # [b, h*w, m]

        mask_in_gtboxes = off_min > 0
        mask_in_level = (off_max > limit_range[0]) & (off_max <= limit_range[1])

        gt_center_x = (gt_boxes[...,0] + gt_boxes[...,2]) / 2
        gt_center_y = (gt_boxes[...,1] + gt_boxes[...,3]) / 2

        radiu = stride * sample_radiu_ratio
        c_left_off = x_list[None,:,None] - gt_center_x[:,None,:]    # [b, h*w, m]
        c_top_off = y_list[None,:,None] - gt_center_y[:,None,:]
        c_right_off =gt_center_x[:,None,:] -  x_list[None,:,None]
        c_bottom_off =gt_center_y[:,None,:] -  y_list[None,:,None]
        c_ltrb_off = torch.stack([c_left_off, c_top_off, c_right_off, c_bottom_off], dim=-1)    # [b, h*w, m, 4]
        c_off_max = torch.max(c_ltrb_off, dim=-1)[0]
        mask_center = c_off_max < radiu

        mask_pos = mask_in_gtboxes & mask_in_level & mask_center

        areas[~mask_pos] = 160000
        areas_min_index = torch.min(areas, dim=-1)[1]   # [b, h*w]
        reg_targets = ltrb_off[torch.zeros_like(areas, dtype=torch.bool).scatter_(-1, areas_min_index.unsqueeze(-1), 1)] # [b*h*w, 4]
        reg_targets = torch.reshape(reg_targets, (batch_size, -1, 4)) # [b, h*w, 4]

        classes = torch.broadcast_tensors(gt_classes[:,None,:], areas.long())[0]    # [b, h*w, m]
        cls_targets = classes[torch.zeros_like(areas, dtype=torch.bool).scatter_(-1, areas_min_index.unsqueeze(-1), 1)]
        cls_targets = torch.reshape(cls_targets, (batch_size, -1, 1)) #[b ,h*w, 1]

        left_right_min = torch.min(reg_targets[...,0], reg_targets[...,2])    # [b, h*w]
        left_right_max = torch.max(reg_targets[...,0], reg_targets[...,2])
        top_bottom_min = torch.min(reg_targets[...,1], reg_targets[...,3])
        top_bottom_max = torch.max(reg_targets[...,1], reg_targets[...,3])
        cnt_targets = ((left_right_min * top_bottom_min) / (left_right_max * top_bottom_max + 1e-10)).sqrt().unsqueeze(dim=-1)   # [b, h*w, 1]

        assert cls_targets.shape == (batch_size, h_mul_w, 1)
        assert cnt_targets.shape == (batch_size, h_mul_w, 1)
        assert reg_targets.shape == (batch_size, h_mul_w, 4)

        # process neg coords
        mask_pos_2 = mask_pos.long().sum(dim=-1)    # [b, h*w]
        mask_pos_2 = mask_pos_2 >= 1
        assert mask_pos_2.shape == (batch_size, h_mul_w)
        cls_targets[~mask_pos_2] = 0
        cnt_targets[~mask_pos_2] = -1
        reg_targets[~mask_pos_2] = -1

        return cls_targets, cnt_targets, reg_targets

    def fmap2orig_coords(self, feature, stride):
        '''
        args:
            feature: [b, c, h, w]
            stride: int
        return:
            coords: [n, 2], Horizontal orientation is first
        '''
        h, w = feature.shape[2: ]
        x_shifts = torch.arange(0, w * stride, stride, dtype=torch.float32)
        y_shifts = torch.arange(0, h * stride, stride, dtype=torch.float32)

        y_shifts, x_shifts = torch.meshgrid(y_shifts, x_shifts)
        x_shifts = torch.reshape(x_shifts, [-1])
        y_shifts = torch.reshape(y_shifts, [-1])
        coords = torch.stack([x_shifts, y_shifts], -1) + stride // 2
        return coords

if __name__=='__main__':
    net1 = GenTargets([8], [[-1, 32]])
    
    import sys
    sys.path.append("..")
    sys.path.append("../..")
    from heads.fcos_head import ClsCntRegHead
    net2 = ClsCntRegHead(256, 8)
    inputs = [torch.rand(3, 256, 100, 100).clone().detach()]
    cls_logits, cnt_logits, reg_predict = net2(inputs)
    out = [cls_logits, cnt_logits, reg_predict]

    from datasets.tod_dataset import todDataset
    dataSet = todDataset('/Users/lhg/Downloads/AI-TOD', 'train')
    imgs, boxes, classes = dataSet.collate_fn([dataSet[106], dataSet[101], dataSet[200]])

    net1_imputs = [out, boxes, classes]
    outputs = net1(net1_imputs)
    print(outputs[0].shape, outputs[1].shape, outputs[2].shape)